#!/usr/bin/env python3
import argparse
import multiprocessing
import os
import subprocess
import sys
from multiprocessing import cpu_count
from shutil import rmtree

import numpy as np
from astropy import units
from astropy.table import Column, Table, vstack
from scipy.integrate import cumtrapz

from table_tools import load_table


def make_columns_file(column_file, temp_table):
    """
    Create a BPZ columns file specifying the input data table columns.

    Parameters
    ----------
    column_file : string
        Path to write the columns file to.
    temp_table : astropy.table.Table
        Table containing the minimal subset of data columns needed by BPZ
    """
    # create a columns file
    with open(column_file, "w") as f:
        for i, (filt, err) in enumerate(zip(args.filters, args.errors)):
            # register the filters
            if filt.endswith("_mag"):
                filt_name = filt[:-4]
            else:
                filt_name = filt
            f.write(
                "%s %d,%d AB %.2f, %.2f\n" % (
                    filt_name, temp_table.index_column(filt) + 1,
                    temp_table.index_column(err) + 1, 0.01, 0.0))
        # add the object index and prior columns
        if args.id is not None:
            f.write("ID %d\n" % (temp_table.index_column(args.id) + 1))
        f.write(
            "M_0 %d\n" % (temp_table.index_column(args.prior_filter) + 1))


def make_thread_tables(temp_table, logdir):
    """
    Create a BPZ columns file specifying the input data table columns.

    Parameters
    ----------
    temp_table : astropy.table.Table
        Table containing the minimal subset of data columns needed by BPZ
    logdir : string
        Folder in which all logs are stored for later inspection.
    """
    cat_files = []
    log_files = []
    # estimate the number of rows per table based on the number of threads
    row_count = len(temp_table) // args.threads + 1
    for i in range(args.threads):
        # take the input file path and add a counter to discriminate each
        # thread's table
        basename = os.path.splitext(args.input)[0] + ".%02d" % (i + 1)
        cat_files.append(basename + ".ascii")
        # use the same naming patterns for the log files, but based on output
        # file name
        log_base = os.path.basename(os.path.splitext(args.output)[0])
        log_files.append(
            os.path.join(logdir, log_base + ".%02d.log" % (i + 1)))
        # split the table into slices of length row_count
        idx_start = i * row_count
        slice_table = temp_table[idx_start:idx_start + row_count]
        # write the splits in ascii format without header
        slice_table.write(
            cat_files[-1], format="ascii.no_header", overwrite=True)
    return cat_files, log_files


def run_BPZ(arguments):
    """
    Wrapper for BPZ used for multiprocessing. Parses all arguments to BPZ and
    redirects the output to a log file (if given) for later inspection.

    Parameters
    ----------
    arguments : list
        List containing the input table file path, the posterior output file
        and the log file path (if None, use stdout).

    Returns
    -------
    outputfile : string
        Path to BPZ output table.
    """
    inputfile, logfile = arguments
    outputfile = os.path.splitext(inputfile)[0] + ".bpz"
    # assemble command chain
    command = [
        "python2", os.path.join(bpz_path, "bpz.py"), inputfile,
        "-COLUMNS", column_file, "-OUTPUT", outputfile,
        "-PRIOR", args.prior, "-SPECTRA", args.templates + ".list",
        "-ZMIN", str(args.z_min), "-ZMAX", str(args.z_max),
        "-INTERP", "10", "-NEW_AB", "no",
        "-ODDS", "0.68", "-MIN_RMS", "0.0", "-INTERACTIVE", "no",
        "-VERBOSE", "yes" if args.verbose else "no",
        "-CHECK", "no"]
    # open the log file
    if logfile is not None:
        with open(logfile, 'w') as log:
            return_code = subprocess.call(command, stdout=log, stderr=log)
    else:
        return_code = subprocess.call(command)
    return outputfile, return_code


if __name__ == "__main__":

    try:
        bpz_path = os.environ["BPZPATH"]
    except KeyError:
        sys.exit(
            "ERROR: $BPZPATH (BPZ source director path) not found in "
            "enironment")
    os.environ["NUMERIX"] = "numpy"  # required for BPZ
    sed_path = os.path.join(bpz_path, "SED")
    # collect a list of installed templates and priors to be displayed in the
    # argument parser help text.
    template_list = [
        os.path.splitext(p)[0] for p in os.listdir(sed_path)
        if p.endswith(".list")]
    prior_list = [
        os.path.splitext(p)[0].split("prior_")[1] for p in os.listdir(bpz_path)
        if p.startswith("prior_") and p.endswith(".py")]

    parser = argparse.ArgumentParser(
        description='Wrapper for BPZ that implements multithreading and '
                    'support for a wider range of table input formats. Input '
                    'data is split into chunks which are processed in '
                    'parallel.')

    data_group = parser.add_argument_group('data')
    data_group.add_argument(
        '-i', '--input', required=True, help='file path of input data table')
    data_group.add_argument(
        '--i-format', default='fits',
        help='astropy.table format specifier of the input table '
             '(default: %(default)s)')
    data_group.add_argument(
        '-o', '--output', required=True, help='file path of output table')
    data_group.add_argument(
        '--o-format', default='fits',
        help='astropy.table format specifier of the output table '
             '(default: %(default)s)')

    params_group = parser.add_argument_group('parameters')
    params_group.add_argument(
        '--filters', nargs='*', required=True,
        help='list of table column names defining the magnitude columns')
    params_group.add_argument(
        '--errors', nargs='*', required=True,
        help='list of table column names defining the magitude errors')
    params_group.add_argument(
        '--prior-filter', required=True,
        help='table column name of filter used to evaluate the BPZ prior')

    bpz_group = parser.add_argument_group('BPZ')
    bpz_group.add_argument(
        '--templates', choices=template_list, default='CWWSB4',
        help='template set list defined in the SED folder of BPZ '
             '(default: %(default)s)')
    bpz_group.add_argument(
        '--prior', choices=prior_list, default='hdfn_gen',
        help='prior function defined in the BPZ root-folder '
             '(default: %(default)s)')
    bpz_group.add_argument(
        '--id', help='table column name of a unique object identifier')
    bpz_group.add_argument(
        '--z-min', type=float, default=0.01,
        help='minimum allowed redshift (default: %(default)s)')
    bpz_group.add_argument(
        '--z-max', type=float, default=7.0,
        help='maximum allowed redshift (default: %(default)s)')
    bpz_group.add_argument(
        '--threads', type=int, default=cpu_count(),
        help='number of threads to use (default: %(default)s)')
    bpz_group.add_argument(
        '-v', '--verbose', action='store_true',
        help='show full output from BPZ')

    args = parser.parse_args()

    setattr(args, "threads", min(cpu_count(), max(0, args.threads)))

    if len(args.filters) != len(args.errors):
        sys.exit("ERROR: number of --filters and --errors do not match")
    # check if the filters have existing transmission profiles
    for filt in args.filters:
        # the filter does not care about magnification, thus drop the suffix
        if filt.endswith("_mag"):
            filt_name = filt[:-4]
        else:
            filt_name = filt
        res_file = os.path.join(bpz_path, "FILTER", "%s.res" % filt_name)

    # load the table and check that all required colums exist
    columns = [args.prior_filter]
    columns.extend(args.filters)
    columns.extend(args.errors)
    data = load_table(args.input, args.i_format, columns)

    # create a subdirectory to collect the log files of each thread
    logdir = os.path.join(os.path.dirname(args.input), "BPZ_logs")
    if os.path.exists(logdir):
        rmtree(logdir)
    os.mkdir(logdir)

    string = "build BPZ input catalogues"
    if args.threads > 1:
        string += " for parallel processing"
    print(string)
    # collect only the required data columns
    temp_columns = [] if args.id is None else [args.id]
    temp_columns.extend(args.filters)
    temp_columns.extend(args.errors)
    if args.prior_filter not in temp_columns:
        temp_columns.append(args.prior_filter)
    temp_table = data[temp_columns]

    # create write a temporary table(s per thread) in ascii format
    column_file = os.path.splitext(args.input)[0] + ".columns"

    # put this in a try statement to clean up the temporary files if something
    # goes wrong
    try:
        make_columns_file(column_file, temp_table)
        # write a table-subset for each BPZ thread
        cat_files, log_files = make_thread_tables(
            temp_table, logdir)
        del(temp_table)  # free some memory since these are now on disk
        # run bpz
        message = "running BPZ"
        if args.threads > 1:
            print(message + " with %d threads" % args.threads)
            print(
                "redirecting concurrent output to: %s" %
                log_files[0].replace(".01.", ".*."))
            with multiprocessing.Pool(args.threads) as pool:
                results = pool.map(
                    run_BPZ, list(zip(cat_files, log_files)))
            out_files = [res[0] for res in results]
            return_codes = [res[1] for res in results]
        else:
            print(message)
            # if there is only one thread the output will not be stored in a
            # log file
            print("#" * 30)
            out_file, return_code = run_BPZ([cat_files[0], None])
            print("#" * 30)
            out_files = [out_file]
            return_codes = [return_code]

        # check if BPZ terminated with code 0
        for return_code, out_file in zip(return_codes, out_files):
            if return_code != 0:
                threadID = out_file.rsplit(".")[1]
                sys.exit(
                    "ERROR: BPZ in thread %s exited with return-code %d" % (
                        threadID, int(return_code)))

        # recombine and convert output tables
        print(
            "%s output data" % (
                "concatenate" if args.threads > 1 else "convert"))
        # Merge back the BPZ output table(s). Read the output files from disk
        # and delete them afterwards.
        tables = []
        for fpath in out_files:
            tables.append(Table.read(fpath, format="ascii"))
            try:
                os.remove(fpath)
            except Exception:
                pass
        table = vstack(tables)
        # correct the index offset (each threads starts indexing from 0)
        if args.id is None:
            table["ID"] = np.arange(1, len(table["ID"]) + 1, dtype=np.int64)

    # cleaning up code, always executed before leaving the script
    finally:
        # try to delete whatever temporary files might exist
        print("remove temporary data")
        try:
            os.remove(column_file)
        except Exception:
            pass
        # remove remaining data products
        for flist in [cat_files, out_files]:
            for fpath in flist:
                try:
                    os.remove(fpath)
                except Exception:
                    pass
