#!/usr/bin/env python3
import argparse
import os
import sys

from memmap_table import MemmapTable

import mock_processing as mocks
from mock_processing.core.utils import ProgressBar
from mock_processing.core.readwrite import guess_format, SUPPORTED_READERS


parser = argparse.ArgumentParser(
    description="Convert a data set to an internal data store.")

parser.add_argument(
    "datastore", help="directory in which the data store is created")
parser.add_argument(
    "--purge", action="store_true",
    help="delete any existing data in datastore")

input_group = parser.add_argument_group("input simulation table")
input_group.add_argument(
    "-i", "--input", required=True, help="path to input simulation table")
input_group.add_argument(
    "--format", choices=sorted(SUPPORTED_READERS.keys()),
    help="input table file format (default: inferred from file extension)")
input_group.add_argument(
    "--fits-ext", type=int, default=1,
    help="FITS table extension to read (default: %(default)s)")


def main():

    args = parser.parse_args()
    logger = mocks.PipeLogger(__file__)

    # automatically determine the input file format
    if args.format is None:
        try:
            args.format = guess_format(args.input)
        except NotImplementedError as e:
            logger.handleException(e)
    message = "opening input as {:}: {:}".format(
        args.format.upper(), args.input)
    logger.info(message)

    # create a standardized input reader
    reader_class = SUPPORTED_READERS[args.format]
    try:
        reader = reader_class(args.input, ext=args.fits_ext)
    except Exception as e:
        logger.handleException(e)
    message = "buffer size: {:.2f} MB ({:,d} rows)".format(
        reader.buffersize / 1024**2, reader.bufferlength)
    logger.debug(message)

    # read the data file and write it to the memmory mapped data store
    with reader:

        with mocks.DataStore.create(
                args.datastore, len(reader), args.purge, logger=logger) as ds:

            # create the new data columns
            logger.debug(
                "registering {:,d} new columns".format(len(reader.colnames)))
            for colname in reader.colnames:
                ds.add_column(colname, dtype=reader.dtype[colname])

            # copy the data
            logger.info("processing input stream ...")
            pbar_kwargs = {
                "leave": False, "unit_scale": True, "unit": "row",
                "dynamic_ncols": True}
            if hasattr(reader, "_guess_length"):
                pbar = ProgressBar()
            else:
                pbar = ProgressBar(n_rows=len(reader))
            # read the data in chunks and copy them to the data store
            start = 0
            for chunk in reader:
                end = reader.current_row  # index where reading will continue
                # map column by column onto the output table
                for colname in reader.colnames:
                    # the CSV reader provides a conservative estimate of the
                    # required number of rows so we need to keep allocating
                    # more memory if necessary
                    if end > len(ds):
                        ds.expand(len(chunk))
                    ds[colname][start:end] = chunk[colname]
                # update the current row index
                pbar.update(end - start)
                start = end
            pbar.close()

            # if using the CSV reader: truncate any allocated, unused rows
            if len(ds) > end:
                ds.resize(end)

            # print a preview of the table as quick check
            try:
                preview = str(ds)
                sys.stdout.write("\n" + preview + "\n\n")
            except Exception:
                logger.warn("table preview failed")

            message = "finalized data store with {:,d} rows ({:})"
            logger.info(message.format(end, ds.filesize))


if __name__ == "__main__":
    main()
