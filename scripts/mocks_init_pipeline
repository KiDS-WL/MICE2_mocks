#!/usr/bin/env python3
import argparse
import os
import sys

from memmap_table import MemmapTable
from mock_processing import PipeLogger, expand_path
from mock_processing.config import DumpColumnMap, load_column_map
from mock_processing.readwrite import guess_format, supported_readers
from mock_processing.utils import (ModificationStamp, ProgressBar,
                                   bytesize_with_prefix, create_column)


parser = argparse.ArgumentParser(
    description="Convert a data set to an internal data store.",
    add_help=False)
parser.register("action", "dump", DumpColumnMap)

parser.add_argument(
    "datastore", type=expand_path,
    help="directory in which the data store is created")
parser.add_argument(
    "--purge", action="store_true",
    help="delete any existing data in datastore")

input_group = parser.add_argument_group("input simulation table")
input_group.add_argument(
    "-i", "--input", required=True, type=expand_path,
    help="path to input simulation table")
input_group.add_argument(
    "--format", choices=sorted(supported_readers.keys()),
    help="input table file format (default: inferred from file extension)")
input_group.add_argument(
    "-c", "--columns", required=True,
    help="parameter file in TOML format that maps the input data column "
         "names to file paths within the data store (use --dump to obtain an "
         "empty parameter file)")
input_group.add_argument(
    "--fits-ext", type=int, default=1,
    help="FITS table extension to read (default: %(default)s)")

help_group = parser.add_argument_group("help and documentation")
help_group.add_argument(
    "-h", "--help", action="help",
    help="show this help message and exit")
help_group.add_argument(
    "--dump", nargs=0, action="dump",
    help="dump a default columns file to stdout and exit")


def main():

    args = parser.parse_args()
    logger = PipeLogger(__file__, args.datastore, append=False)
    timestamp = ModificationStamp(sys.argv)

    # check the columns file
    col_map_dict = load_column_map(args.columns, logger)

    # automatically determine the input file format
    if args.format is None:
        try:
            args.format = guess_format(args.input)
        except NotImplementedError as e:
            logger.handleException(e)
    message = "opening input as {:}: {:}".format(
        args.format.upper(), args.input)
    logger.info(message)

    # create a standardized input reader
    reader_class = supported_readers[args.format]
    try:
        reader = reader_class(
            args.input, datasets=set(col_map_dict.values()),
            ext=args.fits_ext)
    except Exception as e:
        logger.handleException(e)
    message = "buffer size: {:.2f} MB ({:,d} rows)".format(
        reader.buffersize / 1024**2, reader.bufferlength)
    logger.debug(message)

    # read the data file and write it to the memmory mapped data store
    with reader:

        # create the memmory mapped data store
        exists = os.path.exists(args.datastore)
        if not args.purge and exists:
            # this is a safety feature since any existing output directory is
            # erased completely
            message = "ouput path exists but overwriting is not permitted: {:}"
            message = message.format(args.datastore)
            logger.handleException(OSError(message))
        logger.info("creating data store: {:}".format(args.datastore))
        try:
            table = MemmapTable(args.datastore, nrows=len(reader), mode="w+")
        except Exception as e:
            logger.handleException(e)

        with table:
            # create the new data columns
            logger.debug(
                "registering {:,d} new columns".format(len(col_map_dict)))
            for path, (colname, dtype) in col_map_dict.items():
                try:
                    if dtype is None:
                        dtype = reader.dtype[colname].str
                    #column = table.add_column(path, dtype, attr={
                    #    "source file": args.input,
                    #    "source name": colname})
                    column = create_column(
                        table, logger, path, dtype=dtype, attr={
                            "source file": args.input,
                            "source name": colname})
                    timestamp.register(column)
                except KeyError as e:
                    logger.handleException(e)
            logger.debug("allocated {:,d} table rows".format(len(table)))

            # copy the data
            logger.info("processing input stream ...")
            pbar_kwargs = {
                "leave": False, "unit_scale": True, "unit": "row",
                "dynamic_ncols": True}
            if hasattr(reader, "_guess_length"):
                pbar = ProgressBar()
            else:
                pbar = ProgressBar(n_rows=len(reader))
            # read the data in chunks and copy them to the data store
            start = 0
            for chunk in reader:
                end = reader.current_row  # index where reading will continue
                # map column by column onto the output table
                for path, (colname, dtype) in col_map_dict.items():
                    # the CSV reader provides a conservative estimate of the
                    # required number of rows so we need to keep allocating
                    # more memory if necessary
                    if end > len(table):
                        # resize the table
                        table.resize(len(table) + len(chunk))
                        logger.debug("allocated {:,d} additional rows".format(
                            len(chunk)))
                    table[path][start:end] = chunk[colname]
                # update the current row index
                pbar.update(end - start)
                start = end
            pbar.close()

            # if using the CSV reader: truncate any allocated, unused rows
            if len(table) > end:
                table.resize(end)

            # print a preview of the table as quick check
            try:
                preview = str(table)
                sys.stdout.write("\n" + preview + "\n\n")
            except Exception:
                logger.warn("table preview failed")

            logger.info("updating headers and closing data store")
            timestamp.finalize()
            filesize = bytesize_with_prefix(table.nbytes)
        # close the table and flush data
        message = "finalized data store with {:} ({:,d} rows)"
        logger.info(message.format(filesize, end))


if __name__ == "__main__":
    main()
