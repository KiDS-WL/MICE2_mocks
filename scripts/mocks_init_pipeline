#!/usr/bin/env python3
import argparse
import os
import sys

import toml

from memmap_table import MemmapTable
from mock_processing import PipeLogger, expand_path
from mock_processing.readwrite import guess_format, supported_readers
from mock_processing.utils import ColumnDictTranslator, ModificationStamp


class DumpDefault(argparse.Action):

    def __init__(self, *args, nargs=0,**kwargs):
        super().__init__(*args, nargs=nargs, **kwargs)

    def __call__(self, *args, **kwargs):
        # TODO: need to add documentation through comments
        default = {
            "index": "...",
            "position": {
                "ra/true": "",
                "ra/obs": "",
                "dec/true": "",
                "dec/obs": "",
                "z/true": "",
                "z/obs": "",
            },
            "mags/model": {},
        }
        sys.stdout.write(toml.dumps(default))
        parser.exit()


parser = argparse.ArgumentParser(
    description="Convert a data set to an internal data store.",
    add_help=False)
parser.register("action", "dump", DumpDefault)

parser.add_argument(
    "datastore", type=expand_path,
    help="directory in which the data store is created")
parser.add_argument(
    "--purge", action="store_true",
    help="delete any existing data in datastore")

input_group = parser.add_argument_group("input simulation table")
input_group.add_argument(
    "-i", "--input", required=True, type=expand_path,
    help="path to input simulation table")
input_group.add_argument(
    "--format", choices=sorted(supported_readers.keys()),
    help="input table file format (default: inferred from file extension)")
input_group.add_argument(
    "-c", "--columns", required=True,
    help="parameter file in TOML format that maps the input data column "
         "names to file paths within the data store (use --dump to obtain an "
         "empty parameter file)")
input_group.add_argument(
    "--fits-ext", type=int, default=1,
    help="FITS table extension to read (default: %(default)s)")

help_group = parser.add_argument_group("help and documentation")
help_group.add_argument(
    "-h", "--help", action="help",
    help="show this help message and exit")
help_group.add_argument(
    "--dump", nargs=0, action="dump",
    help="dump a default columns file to stdout and exit")


def main():

    args = parser.parse_args()
    logger = PipeLogger(__file__, args.datastore, append=False)
    timestamp = ModificationStamp(sys.argv)

    # check the columns file
    message = "reading columns file: {:}".format(args.columns)
    logger.info(message)
    try:
        with open(args.columns) as f:
            col_map_dict = ColumnDictTranslator(toml.load(f)).column_map
    except OSError as e:
        message = "columns file not found: {:}".format(args.columns)
        logger.handleException(e, message)
    except Exception as e:
        message = "malformed columns file"
        logger.handleException(e, message)

    # automatically determine the input file format
    if args.format is None:
        try:
            args.format = guess_format(args.input)
        except NotImplementedError as e:
            logger.handleException(e)
    message = "opening input as {:}: {:}".format(
        args.format.upper(), args.input)
    logger.info(message)

    # create a standardized input reader
    reader_class = supported_readers[args.format]
    try:
        reader = reader_class(args.input, ext=args.fits_ext)
    except Exception as e:
        logger.handleException(e)

    # read the data file and write it to the memmory mapped data store
    with reader:

        # create the memmory mapped data store
        exists = os.path.exists(args.datastore)
        if not args.purge and exists:
            # this is a safety feature since any existing output directory is
            # erased completely
            message = "ouput path exists but overwriting is not permitted: {:}"
            message = message.format(args.datastore)
            logger.handleException(OSError(message))
        logger.info("creating data store: {:}".format(args.datastore))
        try:
            table = MemmapTable(args.datastore, nrows=len(reader), mode="w+")
        except Exception as e:
            logger.handleException(e)

        with table:
            # create the new data columns
            logger.debug(
                "registering {:,d} new columns".format(len(col_map_dict)))
            for path, (colname, dtype) in col_map_dict.items():
                try:
                    if dtype is None:
                        dtype = reader.dtype[colname].str
                    column = table.add_column(path, dtype, attr={
                        "source file": args.input,
                        "source name": colname})
                    timestamp.register(column)
                except KeyError as e:
                    logger.handleException(e)
            logger.debug("allocated {:,d} table rows".format(len(table)))

            # copy the data
            logger.info("processing input stream ...")
            start = 0
            row_message = "current row: {:,d}".format(start)
            print(row_message, end="\r")
            # read the data in chunks and copy them to the data store
            for chunk in reader:
                end = reader.current_row  # index where reading will continue
                # map column by column onto the output table
                for path, (colname, dtype) in col_map_dict.items():
                    # the CSV reader provides a conservative estimate of the
                    # required number of rows so we need to keep allocating
                    # more memory if necessary
                    if end > len(table):
                        # resize the table
                        table.resize(len(table) + len(chunk))
                        logger.debug(
                            "allocated {:,d} extra rows".format(len(chunk)))
                    table[path][start:end] = chunk[colname]
                row_message = "current row: {:,d}".format(end)
                print(row_message, end="\r")
                # update the current row index
                start = end
            print(" " * len(row_message))  # clear the "current row ..." line

            # if using the CSV reader: truncate any allocated, unused rows
            if len(table) > end:
                table.resize(end)

            # print a preview of the table as quick check
            print(str(table) + "\n")

        # close the table
        timestamp.finalize()
        logger.info("finalized data store with {:,d} rows".format(end))


if __name__ == "__main__":
    main()
