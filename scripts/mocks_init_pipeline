#!/usr/bin/env python3
import argparse
import os
import sys

from memmap_table import MemmapTable

import mock_processing as mocks
from mock_processing.core.config import DumpColumnMap, load_column_map
from mock_processing.core.readwrite import guess_format, SUPPORTED_READERS
from mock_processing.core.utils import ProgressBar


parser = argparse.ArgumentParser(
    description="Convert a data set to an internal data store.",
    add_help=False)
parser.register("action", "dump", DumpColumnMap)

parser.add_argument(
    "datastore", help="directory in which the data store is created")
parser.add_argument(
    "--purge", action="store_true",
    help="delete any existing data in datastore")

input_group = parser.add_argument_group("input simulation table")
input_group.add_argument(
    "-i", "--input", required=True, help="path to input simulation table")
input_group.add_argument(
    "--format", choices=sorted(SUPPORTED_READERS.keys()),
    help="input table file format (default: inferred from file extension)")
input_group.add_argument(
    "-c", "--columns", required=True,
    help="parameter file in TOML format that maps the input data column "
         "names to file paths within the data store (use --dump to obtain an "
         "empty parameter file)")
input_group.add_argument(
    "--fits-ext", type=int, default=1,
    help="FITS table extension to read (default: %(default)s)")

help_group = parser.add_argument_group("help and documentation")
help_group.add_argument(
    "-h", "--help", action="help",
    help="show this help message and exit")
help_group.add_argument(
    "--dump", nargs=0, action="dump",
    help="dump a default columns file to stdout and exit")


def main():

    args = parser.parse_args()
    logger = mocks.PipeLogger(__file__, args.datastore, append=False)

    # check the columns file
    col_map_dict = load_column_map(args.columns, logger)

    # automatically determine the input file format
    if args.format is None:
        try:
            args.format = guess_format(args.input)
        except NotImplementedError as e:
            logger.handleException(e)
    message = "opening input as {:}: {:}".format(
        args.format.upper(), args.input)
    logger.info(message)

    # create a standardized input reader
    reader_class = SUPPORTED_READERS[args.format]
    try:
        reader = reader_class(
            args.input, datasets=set(col_map_dict.values()),
            ext=args.fits_ext)
    except Exception as e:
        logger.handleException(e)
    message = "buffer size: {:.2f} MB ({:,d} rows)".format(
        reader.buffersize / 1024**2, reader.bufferlength)
    logger.debug(message)

    # read the data file and write it to the memmory mapped data store
    with reader:

        with mocks.DataStore.create(
                args.datastore, len(reader), args.purge, logger=logger) as ds:

            # create the new data columns
            logger.debug(
                "registering {:,d} new columns".format(len(col_map_dict)))
            for path, (colname, dtype) in col_map_dict.items():
                try:
                    if dtype is None:
                        dtype = reader.dtype[colname].str
                    ds.add_column(
                        path, dtype=dtype, attr={
                            "source file": args.input, "source name": colname})
                except KeyError as e:
                    logger.handleException(e)

            # copy the data
            logger.info("converting input data")
            pbar_kwargs = {
                "leave": False, "unit_scale": True, "unit": "row",
                "dynamic_ncols": True}
            if hasattr(reader, "_guess_length"):
                pbar = ProgressBar()
            else:
                pbar = ProgressBar(n_rows=len(reader))
            # read the data in chunks and copy them to the data store
            start = 0
            for chunk in reader:
                end = reader.current_row  # index where reading will continue
                # map column by column onto the output table
                for path, (colname, dtype) in col_map_dict.items():
                    # the CSV reader provides a conservative estimate of the
                    # required number of rows so we need to keep allocating
                    # more memory if necessary
                    if end > len(ds):
                        ds.expand(len(chunk))
                    ds[path][start:end] = chunk[colname]
                # update the current row index
                pbar.update(end - start)
                start = end
            pbar.close()

            # if using the CSV reader: truncate any allocated, unused rows
            if len(ds) > end:
                ds.resize(end)

            # print a preview of the table as quick check
            try:
                preview = str(ds)
                sys.stdout.write("\n" + preview + "\n\n")
            except Exception:
                logger.warn("table preview failed")

            message = "finalized data store with {:,d} rows ({:})"
            logger.info(message.format(end, ds.filesize))


if __name__ == "__main__":
    main()
