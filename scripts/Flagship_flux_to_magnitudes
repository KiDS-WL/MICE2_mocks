#!/usr/bin/env python3
import argparse
import os
import sys

import mock_processing as mocks
from mock_processing.Flagship import flux_to_magnitudes_wrapped


parser = argparse.ArgumentParser(
    description="Convert fluxes in Flagship to AB magnitudes.")
parser.add_argument(
    "datastore", help="directory in which the data store is located")
parser.add_argument(
    "--flux",
    help="sub-directory within data store which contains the input fluxes")
parser.add_argument(
    "--mag",
    help="sub-directory within data store where the AB magnitudes are stored")
parser.add_argument(
    "--threads", type=int, default=4,
    help="number of threads to use (default: %(default)s)")


def main():

    args = parser.parse_args()
    logger = mocks.PipeLogger(__file__, args.datastore)

    # convert model fluxes to model magnitudes
    with mocks.DataStore.open(
            args.datastore, readonly=False, logger=logger) as ds:

        ds.pool.set_worker(flux_to_magnitudes_wrapped)

        # find all flux columns
        try:
            model_fluxes, _ = ds.load_photometry(args.flux)
        except KeyError as e:
            logger.handleException(e)

        # create the output columns
        for key, flux_path in model_fluxes.items():
            mag_path = os.path.join(args.mag, key)
            # create new output columns
            ds.add_column(
                mag_path, dtype=ds[flux_path].dtype.str, overwrite=True,
                attr={
                    "description":
                    "{:} converted to AB magnitudes".format(flux_path)})
            # add columns to call signature
            ds.pool.add_argument_column(flux_path)
            ds.pool.add_result_column(mag_path)

        # compute and store the corrected magnitudes in parallel
        ds.pool.execute(args.threads)
        logger.info("computation completed for {:,d} entries".format(len(ds)))


if __name__ == "__main__":
    main()
