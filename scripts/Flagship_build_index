#!/usr/bin/env python3
import argparse
import os
import sys

from mock_processing import PipeLogger, expand_path, open_datastore
from mock_processing.Flagship import compute_unique_index
from mock_processing.parallel import ParallelTable
from mock_processing.utils import (ModificationStamp, create_column,
                                   require_column)


parser = argparse.ArgumentParser(
    description="Compute a unique galaxy index from the halo and halo-galaxy "
                "indices in Flagship.")
parser.add_argument(
    "datastore", type=expand_path,
    help="directory in which the data store is located")
parser.add_argument(
    "--idx-halo",
    help="column with halo indices")
parser.add_argument(
    "--idx-gal",
    help="column with galaxy indices, counted within the host halo")
parser.add_argument(
    "--idx", default="index",
    help="column where the combined unique galaxy index is stored "
         "(default: %(default)s)")
parser.add_argument(
    "--threads", type=int, default=4,
    help="number of threads to use (default: %(default)s)")


def main():

    args = parser.parse_args()
    logger = PipeLogger(__file__, args.datastore)
    timestamp = ModificationStamp(sys.argv)

    # compute a unique galaxy index by summing the halo and galaxy indices
    with open_datastore(args.datastore, logger, readonly=False) as table:

        pool = ParallelTable(table, logger)
        pool.set_worker(compute_unique_index)

        # find halo index and halo-galaxy index columns
        input_columns = (
            ("halo index", args.idx_halo),
            ("galaxy index", args.idx_gal))
        for col_desc, path in input_columns:
            require_column(table, logger, path, col_desc)
            pool.add_argument_column(path)

        # create the output column
        column = create_column(
            table, logger, args.idx, dtype="i8", attr={
                "description": "unique galaxy index"}, overwrite=True)
        timestamp.register(column)
        # add column to call signature
        pool.add_result_column(args.idx)

        # compute and store the corrected magnitudes in parallel
        pool.execute(args.threads)

        logger.info("updating headers and closing data store")
        timestamp.finalize()
    # close the table and flush data
    logger.info("computation completed for {:,d} entries".format(len(table)))


if __name__ == "__main__":
    main()
