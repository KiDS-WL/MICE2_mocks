#!/usr/bin/env python3
import argparse
import os

import mock_processing as mocks


parser = argparse.ArgumentParser(
    description="Show information about the data store like the size on disk, "
                "shape, column names, descriptions, attributes and pipeline "
                "logs and processing history.")
parser.add_argument(
    "datastore", help="directory in which the data store is created")
parser.add_argument(
    "-c", "--columns", action="store_true", help="show the column data types")
parser.add_argument(
    "-a", "--attr", action="store_true", help="show the column attributes")
parser.add_argument(
    "-p", "--history", action="store_true", help="show the pipeline history")
parser.add_argument(
    "-l", "--logs", action="store_true",
    help="show the attached pipeline logs")


def main():

    args = parser.parse_args()
    with mocks.DataStore.open(args.datastore) as ds:

        # display the table meta data
        print("==> META DATA")
        n_cols, n_rows = ds.shape
        print("root:     {:}".format(ds.root))
        print("size:     {:}".format(ds.filesize))
        print("shape:    {:,d} rows x {:d} columns".format(n_rows, n_cols))

        if args.columns:
            # list all the column names and data types
            header = "==> COLUMN NAME"
            width_cols = max(len(header), max(
                len(colname) for colname in ds.colnames))
            print("\n{:}    {:}".format(header.ljust(width_cols), "TYPE"))
            for name in ds.colnames:
                colname_padded = name.ljust(width_cols)
                line = "{:}    {:}".format(
                    colname_padded, str(ds[name].dtype))
                print(line)

        if args.attr:
            # for each column print a summary of the attached attributes
            print("\n==> ATTRIBUTES")
            for name in ds.colnames:
                print()
                # print the column name indented and then a tree-like listing
                # of the attributes (drawing connecting lines for better
                # visibitilty)
                print("{:}".format(name))
                attrs = ds[name].attr
                # all attributes from the pipeline should be dictionaries
                if type(attrs) is dict:
                    i_last = len(attrs)
                    width_key = max(len(key) + 2 for key in attrs)
                    for i, key in enumerate(sorted(attrs), 1):
                        print_key = key + " :"
                        line = "{:}{:} {:}".format(
                            " └╴ " if i == i_last else " ├╴ ",
                            print_key.ljust(width_key), str(attrs[key]))
                        print(line)
                # fallback
                else:
                    print("     └╴ {:}".format(str(attrs)))

        if args.history:
            # list all pipeline script calls ordered by time
            print("\n==> HISTORY")
            date_width = 24
            for date, call in ds.get_history().items():
                print("{:} : {:}".format(date.ljust(date_width), call))

        if args.logs:
            # show the log file
            print("\n==> LOGS")
            logpath = ds.root + ".log"
            if not os.path.exists(logpath):
                raise OSError("log file not found: {:}".format(logpath))
            with open(logpath) as f:
                for line in f.readlines():
                    print(line.strip())


if __name__ == "__main__":
    main()
