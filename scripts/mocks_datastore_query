#!/usr/bin/env python3
import argparse
import os
import sys
import warnings
from time import asctime

import numpy as np

from memmap_table.mathexpression import MathTerm

import mock_processing as mocks
from mock_processing.core.readwrite import (BUFFERSIZE, guess_format,
                                            supported_writers)
from mock_processing.core.utils import ProgressBar, bytesize_with_prefix


parser = argparse.ArgumentParser(
    description="Select a (sub-)sample of the pipeline output and save it "
                "in a different data format")

parser.add_argument(
    "datastore", help="directory in which the data store is created")
parser.add_argument(
    "--verify", action="store_true",
    help="verify the data integrity of all accessed columns")
parser.add_argument(
    "-q", "--query",
    help="expression to select a subset of the pipeline output "
         "(default: select all entries)")

output_group = parser.add_argument_group("output table")
output_group.add_argument(
    "-o", "--output",
    help="path to output data table (default: write on stdout)")
output_group.add_argument(
    "--format", choices=sorted(supported_writers.keys()),
    help="output table file format (default: inferred from file extension)")
output_group.add_argument(
    "-c", "--columns", nargs="*",
    help="only write these columns to the output table (order preserved)")
output_group.add_argument(
    "--compression", choices=(
        "none", "snappy", "lzf", "gzip", "lzo", "brotli", "lz4", "zstd"),
    help="compression methods used for parquet (default: snappy) and HDF5 "
         "files (default: lzf), only none and gzip is supported by both "
         "formats")
output_group.add_argument(
    "--hdf5-shuffle", action="store_true",
    help="shuffle the HDF5 file data blocks prior to compression")
output_group.add_argument(
    "--hdf5-checksum", action="store_true",
    help="write fletcher32 check sums to HDF5 file data blocks")


def main():

    args = parser.parse_args()
    logger = mocks.PipeLogger(__file__, args.datastore)
    # determine if output goes to stdout in CSV format (allows redirecting)
    to_stdout = args.output is None
    if to_stdout:  # only print in case of error
        logger.setTermLevel("error")

    # read the input table and write the selected entries to the output file
    with mocks.DataStore.open(args.datastore, logger=logger) as ds:

        # parse the math expression
        if args.query is not None:
            selection_columns = set()
            # Since the symbol / can be used as column name and division
            # symbol, we need to temporarily substitute the symbol before
            # parsing the expression.
            substitute = "#"
            try:
                # apply the substitutions to all valid column names apperaing
                # in the math expression to avoid substitute intended divisions
                for colname in ds.colnames:
                    substitue = colname.replace("/", substitute)
                    while colname in args.query:
                        args.query = args.query.replace(colname, substitue)
                        selection_columns.add(colname)
                expression = MathTerm.from_string(args.query)
                # recursively undo the substitution
                expression._substitute_characters(substitute, "/")
                # display the interpreted expression
                message = "apply selection: {:}".format(expression.expression)
                logger.info(message)
            except SyntaxError as e:
                message = e.args[0].replace(substitute, "/")
                logger.handleException(SyntaxError(message))
            except Exception as e:
                logger.handleException(e)
            # create a sub-table with the data needed for the selection
            selection_table = ds[sorted(selection_columns)]
        else:
            expression = None
            selection_columns = None

        # check the requested columns
        if args.columns is not None:
            # check for duplicates
            requested_columns = set()
            for colname in args.columns:
                if colname in requested_columns:
                    message = "duplicate column: {:}".format(colname)
                    logger.handleException(KeyError(message))
                requested_columns.add(colname)
            # find requested columns that do not exist in the table
            missing_cols = requested_columns - set(ds.colnames)
            if len(missing_cols) > 0:
                message = "column {:} not found: {:}".format(
                    "name" if len(missing_cols) == 1 else "names",
                    ", ".join(sorted(missing_cols)))
                logger.handleException(KeyError(message))
            # establish the requried data type
            n_cols = len(requested_columns)
            message = "select a subset of {:d} column".format(n_cols)
            if n_cols > 1:
                message += "s"
            logger.info(message)
            dtype = np.dtype([
                (colname, ds.dtype[colname]) for colname in args.columns])
            # create a table view with only the requested columns
            request_table = ds[args.columns]
        else:
            dtype = ds.dtype
            request_table = ds
        
        # verify the data if requested
        if args.verify:
            logger.info("running SHA-1 checksums")
            if args.columns is None:
                verification_columns = ds.colnames
            else:
                verification_columns = [col for col in args.columns]
                if selection_columns is not None:
                    verification_columns.extend(selection_columns)
            name_width = max(len(name) for name in verification_columns)
            for i, name in enumerate(verification_columns, 1):
                sys.stdout.write("checking {:3d} / {:3d}: {:}\r".format(
                    i, len(verification_columns), name.ljust(name_width)))
                sys.stdout.flush()
                ds.verify_column(name)

        # automatically determine the format file format
        if args.output is None:  # write to stdout in csv format
            args.format = "csv"
        if args.format is None:
            try:
                args.format = guess_format(args.output)
            except NotImplementedError as e:
                logger.handleException(e)
        message = "writing output as {:}: {:}".format(
            args.format.upper(), args.output)
        logger.info(message)

        # create a standardized output writer
        writer_class = supported_writers[args.format]
        try:
            writer = writer_class(
                dtype, args.output, overwrite=True,
                # format specific parameters
                compression=args.compression,
                hdf5_shuffle=args.hdf5_shuffle,
                hdf5_checksum=args.hdf5_checksum)
        except Exception as e:
            logger.handleException(e)

        with writer:

            # determine an automatic buffer/chunk size
            chunksize = BUFFERSIZE // ds.itemsize
            message = "buffer size: {:} ({:,d} rows)".format(
                bytesize_with_prefix(writer.buffersize), writer.bufferlength)
            logger.debug(message)

            # query the table and write to the output fie
            logger.info("processing input stream ...")
            if not to_stdout:
                pbar = ProgressBar(len(ds))
            n_select = 0
            for start, end in ds.row_iter(chunksize):

                # apply optional selection, read only minimal amount of data
                if expression is not None:
                    chunk = selection_table[start:end]
                    try:
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            mask = expression(chunk)
                    except Exception as e:
                        logger.handleException(e)
                    selection = request_table[start:end][mask]
                # read all entries in the range without applying the selection
                else:
                    selection = request_table[start:end]

                # write to target
                writer.write_chunk(selection.to_records())
                n_select += len(selection)
                if not to_stdout:
                    pbar.update(end - start)
            if not to_stdout:
                pbar.close()

        message = "wrote {:,d} matching entries (total: {:,d})"
        logger.info(message.format(n_select, end))


if __name__ == "__main__":
    main()
