#!/usr/bin/env python
import argparse
import os
import sys

import mock_processing as mocks
from mock_processing.core.config import (DumpMatchingConfig,
                                         load_matching_config)
from mock_processing.matching import DataMatcher


parser = argparse.ArgumentParser(
    description="Infer attributes from an external data set using a k-nearest "
                "neighbour matching.",
    epilog="The input data must be loaded to memory at once, building the "
           "search tree is not parallelized. Therefore matching against large "
           "data sets may be slow.",
    add_help=False)
parser.register("action", "dump", DumpMatchingConfig)

parser.add_argument(
    "datastore", help="directory in which the data store is located")
parser.add_argument(
    "-c", "--config", required=True,
    help="parameter file in TOML format that defines the properties of the "
         "photometry realisation and the method parameters (use --dump to "
         "obtain a default parameter file)")
parser.add_argument(
    "--threads", type=int, default=4,
    help="number of threads to use (default: %(default)s)")

help_group = parser.add_argument_group("help and documentation")
help_group.add_argument(
    "-h", "--help", action="help",
    help="show this help message and exit")
help_group.add_argument(
    "--dump", nargs=0, action="dump",
    help="dump a default columns file to stdout and exit")


def main():

    args = parser.parse_args()
    logger = mocks.PipeLogger(__file__, args.datastore)

    # check the configuration file
    config = load_matching_config(args.config, logger)

    # apply the magnification correction to the model magnitudes
    with mocks.DataStore.open(args.datastore, False, logger=logger) as ds:

        with DataMatcher(config, logger) as matcher:

            ds.pool.set_worker(matcher.apply)
            # increase the default chunksize, larger chunks will be marginally
            # faster but the progress update will be infrequent
            ds.pool.chunksize = max(args.threads * 4096, ds.pool.chunksize)

            # select the required feature columns
            for feature_path in config.features.values():
                ds.require_column(feature_path, "feature")
                ds.pool.add_argument_column(feature_path, keyword=feature_path)
            # check that the data types are compatible
            try:
                matcher.check_features(ds)
            except Exception as e:
                logger.handleException(e)

            # make the output columns for each observable
            for output_path, dtype, attr in matcher.observable_information():
                ds.add_column(
                    output_path, dtype=dtype, attr=attr, overwrite=True)
                ds.pool.add_result_column(output_path)

            logger.info(
                "building feature space tree, this may take a while ...")
            matcher.build_tree()

            # compute and store the corrected magnitudes
            ds.pool.add_argument_constant(args.threads, keyword="threads")
            ds.pool.apply()  # cKDTree implementation releases GIL
        logger.info("computation completed for {:,d} entries".format(len(ds)))


if __name__ == "__main__":
    main()
