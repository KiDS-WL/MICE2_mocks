#!/usr/bin/env python3
import argparse
import multiprocessing
import os
import sys
from multiprocessing import cpu_count

import numpy as np

from h5table.memmap import BinaryTable
from MICE2_mocks import pipeline


if __name__ == "__main__":

    parser = argparse.ArgumentParser(
        description='Compute the signal-to-noise ratio correction factor '
                    'for an extended object compared to a point source '
                    'assuming a n=4 Sersic profile for the bulge and a n=1 '
                    'Sersic profile for the disk component. The size is '
                    'defined in terms of a fraction of the total emitted flux '
                    '(i.e. 0.5 for the half-light radius.')
    parser.add_argument(
        'dataset', help='memory mapped table data set with MICE2 raw data')
    parser.add_argument(
        '-c', '--chunksize', type=int, default=10000 * cpu_count(),
        help='row chunk size to process at once (default: %(default)s)')
    parser.add_argument(
        '--flux-frac', type=float, default=0.5,
        help='fraction of total flux emitted from within computed radius '
             '(default: %(default)s)')
    parser.add_argument(
        '--psf', nargs='*', type=float, required=True,
        help='list of point-spread functions in arcsec, '
             'additionally computes observed galaxy sizes (PSF convolved)')
    parser.add_argument(
        '--filters', nargs='*', required=True,
        help='filter names associated with each --psf given (optional, used '
             'to name the output table columns)')
    parser.add_argument(
        '--scale', type=float, default=1.0,
        help='factor to scale the aperture size (default: %(default)s)')
    parser.add_argument(
        '--threads', type=int, default=cpu_count(),
        help='number of threads to use (default: %(default)s)')
    args = parser.parse_args()
    args.threads = min(cpu_count(), max(1, args.threads))

    logger = pipeline.pipe_logger(__file__)

    logger.info("loading data store '{:}'".format(args.dataset))
    with BinaryTable(args.dataset, mode="r+") as table:

        # find the shape columns
        path_dict = {
            "bulge ratio": "bulge_fraction",
            "bulge size": "bulge_length",
            "disk size": "disk_length",
            "axis ratio": "bulge_axis_ratio"}
        for name, path in path_dict.items():
            if path not in table:
                message = "{:} column {:} not found in data set"
                logger.error(message.format(name, path))
                raise KeyError(message.format(name, path))

        # generate list of PSF sizes and output columns names
        if len(args.psf) != len(args.filters):
            message = "length of --filter and --psf lists do not match"
            logger.error(message)
            raise ValueError(message)
        for psf in args.psf:
            if psf <= 0.0:
                message = "PSF size must be positive"
                logger.error(message)
                raise ValueError(message)
        psf_sizes = {
            key: val for key, val in zip(args.filters, args.psf)}
        # create new output columns
        col = table.add_column(
            "R_E", dtype=np.float32, attr={
                "description":
                "effective radius, L(<R_E) = {:2f} L_tot".format(
                    args.flux_frac)})
        col = table.add_column(
            "aper_a_intr", dtype=np.float32, attr={
                "description": "PSF corrected aperture major axis"})
        col = table.add_column(
            "aper_area_intr", dtype=np.float32, attr={
                "description": "PSF corrected aperture area"})

        # compute intrinsic galaxy sizes
        logger.info("computing intrinsic galaxy sizes")
        with multiprocessing.Pool(args.threads) as pool:
            for start, end in table.row_iter(args.chunksize):
                bulge_ratio = table[path_dict["bulge ratio"]][start:end]
                bulge_size = table[path_dict["bulge size"]][start:end]
                disk_size = table[path_dict["disk size"]][start:end]
                ba_ratio = table[path_dict["axis ratio"]][start:end]
                flux_fraction = np.full_like(bulge_ratio, args.flux_frac)
                # compute the radius of galaxies that emits a certain fraction of
                # the total luminosity
                arguments = list(zip(
                    flux_fraction, disk_size, bulge_size, bulge_ratio))
                chunksize = args.chunksize // args.threads + 1
                galaxy_size = pool.map(
                    root_function, arguments, chunksize=chunksize)
                # compute the intrinsic galaxy major and minor axes and area
                galaxy_major = np.asarray(galaxy_size) * args.scale
                galaxy_minor = galaxy_major * ba_ratio
                galaxy_area = np.pi * galaxy_major * galaxy_minor
                # write the data
                table["R_E"][start:end] = galaxy_size
                table["aper_a_intr"][start:end] = galaxy_major
                table["aper_area_intr"][start:end] = galaxy_area

        # compute the observational galaxy sizes utilizing the PSF per filter
        for filt in args.filters:
            logger.info(
                "computing observed galaxy sizes in filter '{:}'".format(filt))
            psf = psf_sizes[filt]
            # create new output columns
            aper_a = table.add_column(
                "aper_a_{:}".format(filt), dtype=np.float32, attr={
                    "description": "aperture major axis"})
            aper_ab = table.add_column(
                "aper_ba_ratio_{:}".format(filt), dtype=np.float32, attr={
                    "description": "aperture minor-to-major axis-ratio"})
            aper_area = table.add_column(
                "aper_area_{:}".format(filt), dtype=np.float32, attr={
                    "description": "aperture area"})
            sn_factor = table.add_column(
                "sn_factor_{:}".format(filt), dtype=np.float32, attr={
                    "description":
                    "signal-to-noise correction factor for extended source"})
            # compute the apertures
            for start, end in table.row_iter(args.chunksize):
                ba_ratio = table[path_dict["axis ratio"]][start:end]
                galaxy_major = table["aper_a_intr"][start:end]
                # "convolution" with the PSF
                observed_major = np.sqrt(galaxy_major**2 + psf**2)
                observed_minor = np.sqrt((galaxy_major * ba_ratio)**2 + psf**2)
                # compute the observed axis ratio
                observed_ba = observed_minor / observed_major
                # compute the aperture area
                observed_area = np.pi * observed_major * observed_minor
                psf_area = np.pi * psf**2
                # compute the S/N correction by comparing the aperture area to the PSF
                sn_weight = np.sqrt(psf_area / observed_area)
                # write the data
                aper_a[start:end] = observed_major
                aper_ab[start:end] = observed_ba
                aper_area[start:end] = observed_area
                sn_factor[start:end] = sn_weight

    logger.info("done")


if __name__ == "__main__":

    parser = argparse.ArgumentParser(
        description='Create a data table with a photometery realisation based '
                    'on a table with simulated model magnitudes and '
                    'observational detection limits.')
    parser.add_argument(
        'dataset', help='memory mapped table data set with MICE2 raw data')
    parser.add_argument(
        '-c', '--chunksize', type=int, default=10000,
        help='row chunk size to process at once (default: %(default)s)')
    parser.add_argument(
        '--filters', nargs='*', required=True,
        help='list of table column names providing model magnitudes')
    parser.add_argument(
        '--limits', nargs='*', type=float, required=True,
        help='magnitude limits for each entry in --filters')
    parser.add_argument(
        '--significance', type=float, default=1.0,
        help='significance of detection against magnitude limits '
             '(default: %(default)s)')
    parser.add_argument(
        '--sn-limit', type=float, default=0.2,
        help='lower numerical limit for the signal-to-noise ratio '
             '(default: %(default)s)')
    parser.add_argument(
        '--sn-detect', type=float, default=1.0,
        help='limiting signal-to-noise ratio for object detection '
             '(default: %(default)s)')
    parser.add_argument(
        '--sn-factors', nargs='*',
        help='list of table column names of correction factors for the '
             'signal-to-noise ratio of extended sources, one '
             'for each --filter')
    parser.add_argument(
        '--seed', default='KV450',
        help='string to seed the random generator (default: %(default)s)')
    args = parser.parse_args()

    logger = pipeline.pipe_logger(__file__)

    with BinaryTable(args.dataset, mode="r+") as table:

        # check if all argument lengths match
        filters = args.filters
        if len(args.limits) != len(filters):
            message = "number of input --limits do not match --filters"
            logger.error(message)
            raise parser.error(message)
        if args.sn_factors is None:
            sn_factors = [None] * len(filters)
        else:
            sn_factors = args.sn_factors
            if len(sn_factors) != len(filters):
                message = "number of input --sn-factors do not match --filters"
                logger.error(message)
                raise parser.error(message)

        # create noise realisations
        SN_limit = args.sn_limit
        SN_detect = args.sn_detect
        non_detection_magnitude = 99.0  # inserted for non-detections
        detect_sigma_to_mag = 2.5 * np.log10(args.significance)
        # reseed the random state -> reproducible results
        hasher = md5(bytes(args.seed, "utf-8"))
        hashval = bytes(hasher.hexdigest(), "utf-8")
        np.random.seed(np.frombuffer(hashval, dtype=np.uint32))

        # compute the photometry realisation
        for mag_path, mag_lim, sn_path in zip(
                args.filters, args.limits, sn_factors):
            logger.info("processing filter '{:}'".format(mag_path))
            # find the correct magnitude path depending on whether
            # magnification was applied or not
            if mag_path.endswith("_evo"):
                obs_path = mag_path.replace("_evo", "_obs")
                err_path = mag_path.replace("_evo", "_obserr")
            else:
                if mag_path.endswith("_mag"):
                    obs_path = mag_path[:-4] + "_obs_mag"
                    err_path = mag_path[:-4] + "_obserr_mag"
                else:
                    obs_path = mag_path + "_obs"
                    err_path = mag_path + "_obserr"
            # create new output columns
            obs_mags = table.add_column(
                obs_path, dtype=table[mag_path].dtype.str, attr={
                    "description": "realisation of model magnitude"})
            obs_mags_err = table.add_column(
                err_path, dtype=table[mag_path].dtype.str, attr={
                    "description": "error of realisation of model magnitude"})

            # compute the magnitude realisation
            for start, end in table.row_iter(args.chunksize):
                if sn_path is None:
                    SN_factor = 1.0
                else:
                    SN_factor = table[sn_path][start:end]

                # compute the S/N of the model magnitudes
                model_mags = table[mag_path][start:end]
                model_SN = 10 ** (-0.4 * (
                        model_mags - mag_lim - detect_sigma_to_mag))
                model_SN *= SN_factor
                model_SN = np.maximum(model_SN, SN_limit)
                model_mags_err = 2.5 / np.log(10.0) / model_SN

                # compute the magnitde realisation and S/N
                real_mags = np.random.normal(
                    model_mags, model_mags_err, size=len(model_SN))
                real_SN = 10 ** (-0.4 * (
                        real_mags - mag_lim - detect_sigma_to_mag))
                real_SN *= SN_factor
                real_SN = np.maximum(real_SN, SN_limit)
                real_mags_err = 2.5 / np.log(10.0) / real_SN

                # set magnitudes of undetected objects and mag < 5.0 to 99.0
                not_detected = (real_SN < SN_detect) | (real_mags < 5.0)
                real_mags[not_detected] = non_detection_magnitude
                real_mags_err[not_detected] = mag_lim - detect_sigma_to_mag
                
                # write to disk
                obs_mags[start:end] = real_mags
                obs_mags_err[start:end] = real_mags_err
